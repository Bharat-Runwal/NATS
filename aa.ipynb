{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_emb_dim=100,\n",
    "        trg_emb_dim=100,\n",
    "        src_hidden_dim=25,\n",
    "        trg_hidden_dim=50,\n",
    "        src_vocab_size=999,\n",
    "        trg_vocab_size=999,\n",
    "        src_pad_token=0,\n",
    "        trg_pad_token=0,\n",
    "        src_nlayer=2,\n",
    "        trg_nlayer=1,\n",
    "        src_bidirect=True,\n",
    "        batch_size=128,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "        super(seq2seqAttention, self).__init__()\n",
    "        # source embedding\n",
    "        self.src_embedding = torch.nn.Embedding(\n",
    "            src_vocab_size,\n",
    "            src_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.src_embedding.weight, \n",
    "            mean=0.0, \n",
    "            std=0.02\n",
    "        )\n",
    "        # targe embedding\n",
    "        self.trg_embedding = torch.nn.Embedding(\n",
    "            trg_vocab_size,\n",
    "            trg_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.trg_embedding.weight,\n",
    "            mean=0.0,\n",
    "            std=0.02\n",
    "        )\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return\n",
    "    \n",
    "model = seq2seqAttention(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=25,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=len(vocab2id),\n",
    "    trg_vocab_size=len(vocab2id),\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    src_bidirect=True,\n",
    "    batch_size=batch_size,\n",
    "    dropout=0.0\n",
    ").cuda()\n",
    "\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size: 80475\n",
      "The number of batches: 1444\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../sum_data/'\n",
    "file_vocab = 'cnn_vocab.txt'\n",
    "file_corpus = 'cnn.txt'\n",
    "n_epoch = 20\n",
    "batch_size = 64\n",
    "\n",
    "vocab2id, id2vocab = construct_vocab(data_dir+'/'+file_vocab)\n",
    "print 'The vocabulary size: {0}'.format(len(vocab2id))\n",
    "\n",
    "n_batch = create_batch_file(file_name='../sum_data/cnn.txt', batch_size=batch_size)\n",
    "print 'The number of batches: {0}'.format(n_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size: 80475\n",
      "The number of batches: 1444\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from lstm2lstm import *\n",
    "from data_utils import *\n",
    "\n",
    "data_dir = '../sum_data/'\n",
    "file_vocab = 'cnn_vocab.txt'\n",
    "file_corpus = 'cnn.txt'\n",
    "n_epoch = 20\n",
    "batch_size = 64\n",
    "\n",
    "vocab2id, id2vocab = construct_vocab(data_dir+'/'+file_vocab)\n",
    "print 'The vocabulary size: {0}'.format(len(vocab2id))\n",
    "\n",
    "n_batch = create_batch_file(file_name='../sum_data/cnn.txt', batch_size=batch_size)\n",
    "print 'The number of batches: {0}'.format(n_batch)\n",
    "\n",
    "model = seq2seq(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=25,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=len(vocab2id),\n",
    "    trg_vocab_size=len(vocab2id),\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    src_bidirect=True,\n",
    "    batch_size=batch_size,\n",
    "    dropout=0.0\n",
    ").cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('../sum_data/lstm2lstm_results/lstm2lstm_15_1400.model'))\n",
    "\n",
    "src_var, trg_input_var, trg_output_var = process_minibatch(\n",
    "    100, vocab2id, max_lens=[512, 64]\n",
    ")\n",
    "\n",
    "trg_input_arr = [[vocab2id['<s>'] for k in range(64)] for j in range(batch_size)]\n",
    "trg_input_var = Variable(torch.LongTensor(trg_input_arr))\n",
    "for k in range(64):\n",
    "    print k,\n",
    "    logits = model(src_var.cuda(), trg_input_var.cuda())\n",
    "    word_prob = model.decode(logits).data.cpu().numpy().argmax(axis=-1)\n",
    "    for j in range(64):\n",
    "        trg_input_arr[j][k] = word_prob[j][k]\n",
    "        trg_input_var = Variable(torch.LongTensor(trg_input_arr))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "york the the he `` `` `` `` `` `` `` the `` the `` the the the but obama 's the he obama `` `` `` but `` the will not world the he `` the but his the the not world < `` the but his the the not world the the <unk>\n",
      "--------------------------------------------------\n",
      "new : tutu urges <unk> to cool burial dispute . mandela 's family had weighed taking him off life support , document states . court papers filed in a dispute over burials call mandela 's health `` perilous '' . three of mandela 's deceased children are reburied at his home in <unk> . </s>\n",
      "--------------------------------------------------\n",
      "<s> former south african president nelson mandela 's health had declined so sharply last week that his family was considering whether to take him off life support before his condition improved , a court document released thursday revealed . the document , known as a `` certificate of urgency , '' was filed on june 26 . it stated that the 94-year-old mandela `` has taken a turn for the worst and that the mandela family have been advised by the medical practitioners that his life support machine should be switched off . '' `` rather than prolonging his suffering , the mandela family is exploring this option as a very real probability , '' it added . the following day , however , south african president jacob zuma announced that mandela 's condition had improved from critical to critical but stable . mandela remained critical but stable thursday , zuma 's office reported after he visited mandela . it denied reports that mandela was in a `` vegetative state . '' considered the founding father of south africa 's modern democracy , mandela has been hospitalized in pretoria since june 8 for a recurring lung infection -- a legacy of his years of imprisonment under south africa 's now-defunct apartheid regime . another document filed by his family in a burial dispute described his health as `` perilous '' and stated that fears that his death is drawing near are justified . that affidavit was lodged in court this week as 16 members of mandela 's family battled his grandson , nkosi <unk> mandela , also known as mandla , over where three of the anti-apartheid icon 's deceased children should be buried . mandla mandela lost his case wednesday , which meant the return of the remains of the three relatives to the family graveyard in qunu , nelson mandela 's boyhood home , could go ahead . they were reburied thursday in the family compound . but the dispute brought a public chiding from another hero of the anti-apartheid cause , retired archbishop desmond tutu . in a statement carried by the south african press association late thursday , tutu urged the family not to `` <unk> '' mandela 's name in his last days . `` please , please , please may we think not only of ourselves . it 's almost like spitting in madiba 's face , '' tutu said , using mandela 's traditional clan name . `` your anguish , now , is the nation 's anguish -- and the world 's . we want to embrace you , to support you , to shine our love for madiba through you . '' the remains were removed from the qunu village cemetery two years ago by mandla mandela and then taken to the village of <unk> . the former south african president was born in <unk> , but spent his childhood in qunu . messages for nelson mandela : hope turns to resignation the family affidavit lodged at the mthatha high court gives\n"
     ]
    }
   ],
   "source": [
    "idid = 32\n",
    "sen_pred = [id2vocab[x] for x in word_prob[idid]]\n",
    "print ''.join(['-' for k in range(50)])\n",
    "st_idx = len(sen_pred)\n",
    "for k, wd in enumerate(sen_pred):\n",
    "    if wd == '</s>':\n",
    "        st_idx = k\n",
    "        break\n",
    "sen_pred = sen_pred[:st_idx]\n",
    "print ' '.join(sen_pred)\n",
    "\n",
    "print ''.join(['-' for k in range(50)])\n",
    "sen_abs = [id2vocab[x] for x in trg_output_var.data[idid]]\n",
    "st_idx = len(sen_abs)\n",
    "for k, wd in enumerate(sen_abs):\n",
    "    if wd == '<pad>':\n",
    "        st_idx = k\n",
    "        break\n",
    "print ' '.join(sen_abs[:st_idx])\n",
    "\n",
    "print ''.join(['-' for k in range(50)])\n",
    "sen_source = [id2vocab[x] for x in src_var.data[idid]]\n",
    "st_idx = len(sen_source)\n",
    "for k, wd in enumerate(sen_source):\n",
    "    if wd == '<pad>':\n",
    "        st_idx = k\n",
    "        break\n",
    "print ' '.join(sen_source[:st_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

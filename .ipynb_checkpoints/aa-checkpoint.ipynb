{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 5])\n",
      "torch.Size([3, 128, 5])\n",
      "torch.Size([128, 5])\n",
      "torch.Size([128, 5])\n",
      "torch.Size([128, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.LSTM(9, 5, num_layers=3, bidirectional=False, batch_first=True)\n",
    "\n",
    "input = Variable(torch.randn(128, 3, 9))\n",
    "h0 = Variable(torch.randn(3, 128, 5))\n",
    "c0 = Variable(torch.randn(3, 128, 5))\n",
    "output, hn = rnn(input, (h0, c0))\n",
    "hh, cc = hn\n",
    "print hh.size()\n",
    "print cc.size()\n",
    "print hh[-1].size()\n",
    "print cc[-1].size()\n",
    "print output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SoftDotAttention (\n",
      "  (linear_in): Linear (100 -> 100)\n",
      "  (linear_out): Linear (200 -> 100)\n",
      ")\n",
      "Variable containing:\n",
      " 0.0472  0.1069  0.4855  ...  -0.0105  0.2764 -0.2282\n",
      " 0.2576 -0.2164  0.2461  ...   0.0927  0.3746 -0.0385\n",
      " 0.2283 -0.0510  0.1408  ...   0.0626  0.4829 -0.0880\n",
      "          ...             ⋱             ...          \n",
      " 0.0657 -0.0529  0.3123  ...  -0.0555  0.4422 -0.1950\n",
      " 0.2297 -0.1922  0.1218  ...   0.1817  0.2635 -0.2318\n",
      " 0.0733  0.0217  0.1559  ...  -0.1520  0.4014 -0.1698\n",
      "[torch.cuda.FloatTensor of size 128x100 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.0263  0.0275  0.0647  ...   0.0797  0.0287  0.0307\n",
      " 0.3163  0.0128  0.0127  ...   0.0084  0.0196  0.0097\n",
      " 0.0356  0.0361  0.0644  ...   0.0191  0.2002  0.0407\n",
      "          ...             ⋱             ...          \n",
      " 0.0826  0.1428  0.0311  ...   0.0354  0.0274  0.0386\n",
      " 0.0310  0.0753  0.0214  ...   0.1289  0.1041  0.1391\n",
      " 0.0825  0.1653  0.1374  ...   0.0517  0.0853  0.0218\n",
      "[torch.cuda.FloatTensor of size 128x16 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class SoftDotAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(SoftDotAttention, self).__init__()\n",
    "        \n",
    "        self.linear_in = torch.nn.Linear(\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            bias=False\n",
    "        ).cuda()\n",
    "        self.linear_out = torch.nn.Linear(\n",
    "            hidden_size*2,\n",
    "            hidden_size,\n",
    "            bias=False\n",
    "        ).cuda()\n",
    "        \n",
    "    def forward(self, input_, encoder_hy):\n",
    "        \n",
    "        target = self.linear_in(input_).unsqueeze(2)\n",
    "        attn = torch.bmm(encoder_hy, target).squeeze(2)\n",
    "        attn = torch.nn.Softmax()(attn)\n",
    "        attn2 = attn.view(attn.size(0), 1, attn.size(1))\n",
    "        \n",
    "        weighted_context = torch.bmm(attn2, encoder_hy).squeeze(1)\n",
    "        \n",
    "        h_attn = torch.cat((weighted_context, input_), 1)\n",
    "        h_attn = self.linear_out(h_attn)\n",
    "        h_attn = torch.nn.Tanh()(h_attn)\n",
    "        \n",
    "        return h_attn, attn\n",
    "\n",
    "rd1 = Variable(torch.rand([128, 100])).cuda()\n",
    "rd2 = Variable(torch.rand([128, 16, 100])).cuda()\n",
    "model = SoftDotAttention(hidden_size=100).cuda()\n",
    "print model\n",
    "hh, cc = model(rd1, rd2)\n",
    "print hh\n",
    "print cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMAttentionDot (\n",
      "  (lstm_input_w): Linear (100 -> 200)\n",
      "  (lstm_hidden_w): Linear (50 -> 200)\n",
      "  (attn_layer): SoftDotAttention (\n",
      "    (linear_in): Linear (50 -> 50)\n",
      "    (linear_out): Linear (100 -> 50)\n",
      "  )\n",
      ")\n",
      "torch.Size([128, 16, 50])\n"
     ]
    }
   ],
   "source": [
    "class LSTMAttentionDot(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers=1,\n",
    "        batch_first=False\n",
    "    ):\n",
    "        super(LSTMAttentionDot, self).__init__()\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.lstm_input_w = torch.nn.Linear(\n",
    "            self.input_size,\n",
    "            self.hidden_size*4\n",
    "        ).cuda()\n",
    "        self.lstm_hidden_w = torch.nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.hidden_size*4\n",
    "        ).cuda()\n",
    "        \n",
    "        self.attn_layer = SoftDotAttention(self.hidden_size).cuda()\n",
    "        \n",
    "    def forward(self, input_, hidden_, encoder_hy):\n",
    "        # user defined lstm with attention\n",
    "        def attn_lstm(input_, hidden_, encoder_hy):\n",
    "            hx, cx = hidden_\n",
    "            gates = self.lstm_input_w(input_) + self.lstm_hidden_w(hx)\n",
    "            ingate, cellgate, forgetgate, outgate = gates.chunk(4,1)\n",
    "            \n",
    "            ingate = torch.nn.Sigmoid()(ingate)\n",
    "            forgetgate = torch.nn.Sigmoid()(forgetgate)\n",
    "            outgate = torch.nn.Sigmoid()(outgate)\n",
    "            cellgate = torch.nn.Tanh()(cellgate)\n",
    "            \n",
    "            cy = forgetgate*cx + ingate*cellgate\n",
    "            hy = outgate*torch.nn.Tanh()(cy)\n",
    "            \n",
    "            h_attn, attn = self.attn_layer(hy, encoder_hy.transpose(0, 1))\n",
    "            \n",
    "            return h_attn, cy\n",
    "        \n",
    "        if self.batch_first:\n",
    "            input_ = input_.transpose(0,1)\n",
    "            \n",
    "        output_ = []\n",
    "        for k in range(input_.size(0)):\n",
    "            hidden_ = attn_lstm(input_[k], hidden_, encoder_hy)\n",
    "            output_.append(hidden_[0])\n",
    "        \n",
    "        len_seq = input_.size()[0]\n",
    "        batch_size, hidden_size = output_[0].size()\n",
    "        output_ = torch.cat(output_, 0).view(\n",
    "            len_seq, \n",
    "            batch_size, \n",
    "            hidden_size\n",
    "        )\n",
    "        \n",
    "        if self.batch_first:\n",
    "            output_ = output_.transpose(0,1)\n",
    "            \n",
    "        return output_, hidden_\n",
    "    \n",
    "rd1 = Variable(torch.rand([128, 16, 100])).cuda()\n",
    "hd2 = Variable(torch.rand([128, 50])).cuda()\n",
    "cd2 = Variable(torch.rand([128, 50])).cuda()\n",
    "hid2 = (hd2, cd2)\n",
    "ctx = Variable(torch.rand([16, 128, 50])).cuda()\n",
    "model = LSTMAttentionDot(\n",
    "    input_size=100, \n",
    "    hidden_size=50, \n",
    "    num_layers=1, \n",
    "    batch_first=True\n",
    ").cuda()\n",
    "print model\n",
    "aa, bb = model(rd1, hid2, ctx)\n",
    "print aa.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seqAttention (\n",
      "  (src_embedding): Embedding(999, 100, padding_idx=0)\n",
      "  (trg_embedding): Embedding(999, 100, padding_idx=0)\n",
      "  (encoder): LSTM(100, 25, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (decoder): LSTMAttentionDot (\n",
      "    (lstm_input_w): Linear (100 -> 200)\n",
      "    (lstm_hidden_w): Linear (50 -> 200)\n",
      "    (attn_layer): SoftDotAttention (\n",
      "      (linear_in): Linear (50 -> 50)\n",
      "      (linear_out): Linear (100 -> 50)\n",
      "    )\n",
      "  )\n",
      "  (encoder2decoder): Linear (50 -> 50)\n",
      "  (decoder2vocab): Linear (50 -> 999)\n",
      ")\n",
      "torch.Size([128, 16, 100])\n",
      "torch.Size([128, 16, 100])\n",
      "128\n",
      "torch.Size([4, 128, 25])\n",
      "torch.Size([4, 128, 25])\n",
      "src_h\n",
      "torch.Size([128, 16, 50])\n",
      "torch.Size([4, 128, 25])\n",
      "torch.Size([4, 128, 25])\n",
      "h_t\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "h0\n",
      "torch.Size([128, 50])\n",
      "torch.Size([128, 50])\n",
      "trg_h\n",
      "torch.Size([128, 16, 50])\n",
      "reshape\n",
      "torch.Size([2048, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       " -1.1094e-02  2.2234e-02  7.0412e-03  ...  -1.3893e-02 -2.1753e-02  4.0296e-03\n",
       " -3.7722e-03  1.8043e-02  6.6307e-03  ...  -1.9105e-02 -2.4647e-02  5.9177e-03\n",
       "  1.0599e-04  1.5275e-02  6.8597e-03  ...  -2.2098e-02 -2.6381e-02  7.0041e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.6942e-03  1.1150e-02  7.8494e-03  ...  -2.6003e-02 -2.9039e-02  8.7229e-03\n",
       "  4.6966e-03  1.1146e-02  7.8501e-03  ...  -2.6006e-02 -2.9041e-02  8.7249e-03\n",
       "  4.6980e-03  1.1143e-02  7.8504e-03  ...  -2.6008e-02 -2.9042e-02  8.7261e-03\n",
       "\n",
       "( 1 ,.,.) = \n",
       " -1.1094e-02  2.2234e-02  7.0412e-03  ...  -1.3893e-02 -2.1753e-02  4.0296e-03\n",
       " -3.7722e-03  1.8043e-02  6.6307e-03  ...  -1.9105e-02 -2.4647e-02  5.9177e-03\n",
       "  1.0599e-04  1.5275e-02  6.8597e-03  ...  -2.2098e-02 -2.6381e-02  7.0041e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.6942e-03  1.1150e-02  7.8494e-03  ...  -2.6003e-02 -2.9039e-02  8.7229e-03\n",
       "  4.6966e-03  1.1146e-02  7.8501e-03  ...  -2.6006e-02 -2.9041e-02  8.7249e-03\n",
       "  4.6980e-03  1.1143e-02  7.8504e-03  ...  -2.6008e-02 -2.9042e-02  8.7261e-03\n",
       "\n",
       "( 2 ,.,.) = \n",
       " -1.1094e-02  2.2234e-02  7.0412e-03  ...  -1.3893e-02 -2.1753e-02  4.0296e-03\n",
       " -3.7722e-03  1.8043e-02  6.6307e-03  ...  -1.9105e-02 -2.4647e-02  5.9177e-03\n",
       "  1.0599e-04  1.5275e-02  6.8597e-03  ...  -2.2098e-02 -2.6381e-02  7.0041e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.6942e-03  1.1150e-02  7.8494e-03  ...  -2.6003e-02 -2.9039e-02  8.7229e-03\n",
       "  4.6966e-03  1.1146e-02  7.8501e-03  ...  -2.6006e-02 -2.9041e-02  8.7249e-03\n",
       "  4.6980e-03  1.1143e-02  7.8504e-03  ...  -2.6008e-02 -2.9042e-02  8.7261e-03\n",
       "... \n",
       "\n",
       "(125,.,.) = \n",
       " -1.1094e-02  2.2234e-02  7.0412e-03  ...  -1.3893e-02 -2.1753e-02  4.0296e-03\n",
       " -3.7722e-03  1.8043e-02  6.6307e-03  ...  -1.9105e-02 -2.4647e-02  5.9177e-03\n",
       "  1.0599e-04  1.5275e-02  6.8597e-03  ...  -2.2098e-02 -2.6381e-02  7.0041e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.6942e-03  1.1150e-02  7.8494e-03  ...  -2.6003e-02 -2.9039e-02  8.7229e-03\n",
       "  4.6966e-03  1.1146e-02  7.8501e-03  ...  -2.6006e-02 -2.9041e-02  8.7249e-03\n",
       "  4.6980e-03  1.1143e-02  7.8504e-03  ...  -2.6008e-02 -2.9042e-02  8.7261e-03\n",
       "\n",
       "(126,.,.) = \n",
       " -1.1094e-02  2.2234e-02  7.0412e-03  ...  -1.3893e-02 -2.1753e-02  4.0296e-03\n",
       " -3.7722e-03  1.8043e-02  6.6307e-03  ...  -1.9105e-02 -2.4647e-02  5.9177e-03\n",
       "  1.0599e-04  1.5275e-02  6.8597e-03  ...  -2.2098e-02 -2.6381e-02  7.0041e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.6942e-03  1.1150e-02  7.8494e-03  ...  -2.6003e-02 -2.9039e-02  8.7229e-03\n",
       "  4.6966e-03  1.1146e-02  7.8501e-03  ...  -2.6006e-02 -2.9041e-02  8.7249e-03\n",
       "  4.6980e-03  1.1143e-02  7.8504e-03  ...  -2.6008e-02 -2.9042e-02  8.7261e-03\n",
       "\n",
       "(127,.,.) = \n",
       " -1.1094e-02  2.2234e-02  7.0412e-03  ...  -1.3893e-02 -2.1753e-02  4.0296e-03\n",
       " -3.7722e-03  1.8043e-02  6.6307e-03  ...  -1.9105e-02 -2.4647e-02  5.9177e-03\n",
       "  1.0599e-04  1.5275e-02  6.8597e-03  ...  -2.2098e-02 -2.6381e-02  7.0041e-03\n",
       "                 ...                   ⋱                   ...                \n",
       "  4.6942e-03  1.1150e-02  7.8494e-03  ...  -2.6003e-02 -2.9039e-02  8.7229e-03\n",
       "  4.6966e-03  1.1146e-02  7.8501e-03  ...  -2.6006e-02 -2.9041e-02  8.7249e-03\n",
       "  4.6980e-03  1.1143e-02  7.8504e-03  ...  -2.6008e-02 -2.9042e-02  8.7261e-03\n",
       "[torch.cuda.FloatTensor of size 128x16x999 (GPU 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class seq2seqAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_emb_dim=100,\n",
    "        trg_emb_dim=100,\n",
    "        src_hidden_dim=50,\n",
    "        trg_hidden_dim=50,\n",
    "        src_vocab_size=999,\n",
    "        trg_vocab_size=999,\n",
    "        src_pad_token=0,\n",
    "        trg_pad_token=0,\n",
    "        src_nlayer=2,\n",
    "        trg_nlayer=1,\n",
    "        batch_first=True,\n",
    "        src_bidirect=True,\n",
    "        batch_size=128,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "        super(seq2seqAttention, self).__init__()\n",
    "        # parameters\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.src_nlayer = src_nlayer\n",
    "        self.trg_nlayer = trg_nlayer\n",
    "        self.batch_first = batch_first\n",
    "        self.src_bidirect = src_bidirect\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.src_num_directions = 1\n",
    "        if self.src_bidirect:\n",
    "            self.src_hidden_dim = src_hidden_dim // 2\n",
    "            self.src_num_directions = 2\n",
    "        \n",
    "        \n",
    "        # source embedding\n",
    "        self.src_embedding = torch.nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.src_embedding.weight, \n",
    "            mean=0.0, \n",
    "            std=0.02\n",
    "        )\n",
    "        # targe embedding\n",
    "        self.trg_embedding = torch.nn.Embedding(\n",
    "            self.trg_vocab_size,\n",
    "            self.trg_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.trg_embedding.weight,\n",
    "            mean=0.0,\n",
    "            std=0.02\n",
    "        )\n",
    "        # encoder\n",
    "        self.encoder = torch.nn.LSTM(\n",
    "            input_size=self.src_emb_dim,\n",
    "            hidden_size=self.src_hidden_dim,\n",
    "            num_layers=self.src_nlayer,\n",
    "            batch_first=self.batch_first,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=self.src_bidirect\n",
    "        ).cuda()\n",
    "        # decoder\n",
    "        self.decoder = LSTMAttentionDot(\n",
    "            input_size=self.trg_emb_dim,\n",
    "            hidden_size=self.trg_hidden_dim,\n",
    "            batch_first=self.batch_first\n",
    "        ).cuda()\n",
    "        \n",
    "        # encoder to decoder\n",
    "        self.encoder2decoder = torch.nn.Linear(\n",
    "            self.src_hidden_dim*self.src_num_directions,\n",
    "            self.trg_hidden_dim\n",
    "        ).cuda()\n",
    "        torch.nn.init.constant(self.encoder2decoder.bias, 0.0)\n",
    "        # decoder to vocab\n",
    "        self.decoder2vocab = torch.nn.Linear(\n",
    "            self.trg_hidden_dim,\n",
    "            self.trg_vocab_size\n",
    "        ).cuda()\n",
    "        torch.nn.init.constant(self.decoder2vocab.bias, 0.0)\n",
    "        \n",
    "    def forward(self, input_src, input_trg):\n",
    "        \n",
    "        src_emb = self.src_embedding(input_src)\n",
    "        trg_emb = self.trg_embedding(input_trg)\n",
    "        \n",
    "        print src_emb.size()\n",
    "        print trg_emb.size()\n",
    "        \n",
    "        batch_size = input_src.size(1)\n",
    "        if self.batch_first:\n",
    "            batch_size = input_src.size(0)\n",
    "            \n",
    "        print batch_size\n",
    "        \n",
    "        h0_encoder = Variable(torch.zeros(\n",
    "            self.encoder.num_layers*self.src_num_directions,\n",
    "            self.batch_size,\n",
    "            self.src_hidden_dim\n",
    "        ), requires_grad=False).cuda()\n",
    "        \n",
    "        c0_encoder = Variable(torch.zeros(\n",
    "            self.encoder.num_layers*self.src_num_directions,\n",
    "            self.batch_size,\n",
    "            self.src_hidden_dim\n",
    "        ), requires_grad=False).cuda()\n",
    "        \n",
    "        print h0_encoder.size()\n",
    "        print c0_encoder.size()\n",
    "        \n",
    "        src_h, (src_h_t, src_c_t) = self.encoder(\n",
    "            src_emb, \n",
    "            (h0_encoder, c0_encoder)\n",
    "        )\n",
    "        \n",
    "        print 'src_h'\n",
    "        print src_h.size()\n",
    "        print src_h_t.size()\n",
    "        print src_c_t.size()\n",
    "        \n",
    "        if self.src_bidirect:\n",
    "            h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1)\n",
    "            c_t = torch.cat((src_c_t[-1], src_c_t[-2]), 1)\n",
    "        else:\n",
    "            h_t = src_h_t[-1]\n",
    "            c_t = src_c_t[-1]\n",
    "            \n",
    "        print 'h_t'\n",
    "        print h_t.size()\n",
    "        print c_t.size()\n",
    "            \n",
    "        decoder_h0 = self.encoder2decoder(h_t)\n",
    "        decoder_h0 = torch.nn.Tanh()(decoder_h0)\n",
    "        decoder_c0 = c_t\n",
    "        \n",
    "        print 'h0'\n",
    "        print decoder_h0.size()\n",
    "        print decoder_c0.size()\n",
    "        \n",
    "        encoder_hy = src_h.transpose(0,1)\n",
    "        \n",
    "        trg_h, (_, _) = self.decoder(\n",
    "            trg_emb,\n",
    "            (decoder_h0, decoder_c0),\n",
    "            encoder_hy\n",
    "        )\n",
    "        \n",
    "        print 'trg_h'\n",
    "        print trg_h.size()\n",
    "        \n",
    "        trg_h_reshape = trg_h.contiguous().view(\n",
    "            trg_h.size(0) * trg_h.size(1),\n",
    "            trg_h.size(2)\n",
    "        )\n",
    "        \n",
    "        print 'reshape'\n",
    "        print trg_h_reshape.size()\n",
    "        \n",
    "        decoder_output = self.decoder2vocab(trg_h_reshape)\n",
    "        decoder_output = decoder_output.view(\n",
    "            trg_h.size(0),\n",
    "            trg_h.size(1),\n",
    "            decoder_output.size(1)\n",
    "        )\n",
    "        \n",
    "        return decoder_output\n",
    "\n",
    "model = seq2seqAttention(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=50,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=999,\n",
    "    trg_vocab_size=999,\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    batch_first=True,\n",
    "    src_bidirect=True,\n",
    "    batch_size=128,\n",
    "    dropout=0.0\n",
    ").cuda()\n",
    "\n",
    "print model\n",
    "\n",
    "sen_in = Variable(torch.LongTensor(128, 16).fill_(10))\n",
    "sen_out = Variable(torch.LongTensor(128, 16).fill_(9))\n",
    "model(sen_in.cuda(), sen_out.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.8480  0.6187  0.9019  0.9255\n",
      "  0.0469  0.8340  0.1802  0.4042\n",
      "  0.5947  0.9032  0.0727  0.3357\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1368  0.6639  0.0902  0.1951\n",
      "  0.9294  0.4551  0.9758  0.7000\n",
      "  0.1181  0.3522  0.6663  0.9516\n",
      "[torch.cuda.FloatTensor of size 2x3x4 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.8480  0.6187  0.9019  0.9255\n",
      " 0.0469  0.8340  0.1802  0.4042\n",
      " 0.5947  0.9032  0.0727  0.3357\n",
      " 0.1368  0.6639  0.0902  0.1951\n",
      " 0.9294  0.4551  0.9758  0.7000\n",
      " 0.1181  0.3522  0.6663  0.9516\n",
      "[torch.cuda.FloatTensor of size 6x4 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.8480  0.6187  0.9019  0.9255\n",
      "  0.0469  0.8340  0.1802  0.4042\n",
      "  0.5947  0.9032  0.0727  0.3357\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.1368  0.6639  0.0902  0.1951\n",
      "  0.9294  0.4551  0.9758  0.7000\n",
      "  0.1181  0.3522  0.6663  0.9516\n",
      "[torch.cuda.FloatTensor of size 2x3x4 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rd1 = Variable(torch.rand([2, 3, 4])).cuda()\n",
    "print rd1\n",
    "rd2 = rd1.view(6, 4)\n",
    "print rd2\n",
    "print rd2.view(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

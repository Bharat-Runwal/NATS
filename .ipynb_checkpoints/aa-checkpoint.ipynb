{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq2seqAttention (\n",
      "  (src_embedding): Embedding(999, 100, padding_idx=0)\n",
      "  (trg_embedding): Embedding(999, 100, padding_idx=0)\n",
      "  (encoder): LSTM(100, 25, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (decoder): LSTMAttentionDot (\n",
      "  )\n",
      "  (encoder2decoder): Linear (50 -> 50)\n",
      "  (decoder2vocab): Linear (50 -> 999)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SoftDotAttention(torch.nn.Module)\n",
    "\n",
    "class LSTMAttentionDot(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers=1,\n",
    "        batch_first=True\n",
    "    ):\n",
    "        super(LSTMAttentionDot, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = num_layers\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "class seq2seqAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_emb_dim=100,\n",
    "        trg_emb_dim=100,\n",
    "        src_hidden_dim=50,\n",
    "        trg_hidden_dim=50,\n",
    "        src_vocab_size=999,\n",
    "        trg_vocab_size=999,\n",
    "        src_pad_token=0,\n",
    "        trg_pad_token=0,\n",
    "        src_nlayer=2,\n",
    "        trg_nlayer=1,\n",
    "        batch_first=True,\n",
    "        src_bidirect=True,\n",
    "        batch_size=128,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "        super(seq2seqAttention, self).__init__()\n",
    "        # parameters\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.src_nlayer = src_nlayer\n",
    "        self.trg_nlayer = trg_nlayer\n",
    "        self.batch_first = batch_first\n",
    "        self.src_bidirect = src_bidirect\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.src_num_directions = 1\n",
    "        if self.src_bidirect:\n",
    "            self.src_hidden_dim = src_hidden_dim // 2\n",
    "            self.src_num_directions = 2\n",
    "        \n",
    "        \n",
    "        # source embedding\n",
    "        self.src_embedding = torch.nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.src_embedding.weight, \n",
    "            mean=0.0, \n",
    "            std=0.02\n",
    "        )\n",
    "        # targe embedding\n",
    "        self.trg_embedding = torch.nn.Embedding(\n",
    "            self.trg_vocab_size,\n",
    "            self.trg_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.trg_embedding.weight,\n",
    "            mean=0.0,\n",
    "            std=0.02\n",
    "        )\n",
    "        # encoder\n",
    "        self.encoder = torch.nn.LSTM(\n",
    "            input_size=self.src_emb_dim,\n",
    "            hidden_size=self.src_hidden_dim,\n",
    "            num_layers=self.src_nlayer,\n",
    "            batch_first=self.batch_first,\n",
    "            dropout=self.dropout,\n",
    "            bidirectional=self.src_bidirect\n",
    "        ).cuda()\n",
    "        # decoder\n",
    "        self.decoder = LSTMAttentionDot(\n",
    "            input_size=self.trg_emb_dim,\n",
    "            hidden_size=self.trg_hidden_dim,\n",
    "            batch_first=self.batch_first\n",
    "        ).cuda()\n",
    "        \n",
    "        # encoder to decoder\n",
    "        self.encoder2decoder = torch.nn.Linear(\n",
    "            self.src_hidden_dim*self.src_num_directions,\n",
    "            self.trg_hidden_dim\n",
    "        ).cuda()\n",
    "        torch.nn.init.constant(self.encoder2decoder.bias, 0.0)\n",
    "        # decoder to vocab\n",
    "        self.decoder2vocab = torch.nn.Linear(\n",
    "            self.trg_hidden_dim,\n",
    "            self.trg_vocab_size\n",
    "        ).cuda()\n",
    "        torch.nn.init.constant(self.decoder2vocab.bias, 0.0)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_src, input_trg):\n",
    "        \n",
    "        src_emb = self.src_embedding(input_src)\n",
    "        trg_emb = self.trg_embedding(input_trg)\n",
    "        \n",
    "        batch_size = input_src.size(1)\n",
    "        if self.batch_first:\n",
    "            batch_size = input_src.size(0)\n",
    "        \n",
    "        \n",
    "        return\n",
    "    \n",
    "\n",
    "model = seq2seqAttention(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=50,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=999,\n",
    "    trg_vocab_size=999,\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    batch_first=True,\n",
    "    src_bidirect=True,\n",
    "    batch_size=128,\n",
    "    dropout=0.0\n",
    ").cuda()\n",
    "\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size: 80475\n",
      "The number of batches: 1444\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../sum_data/'\n",
    "file_vocab = 'cnn_vocab.txt'\n",
    "file_corpus = 'cnn.txt'\n",
    "n_epoch = 20\n",
    "batch_size = 64\n",
    "\n",
    "vocab2id, id2vocab = construct_vocab(data_dir+'/'+file_vocab)\n",
    "print 'The vocabulary size: {0}'.format(len(vocab2id))\n",
    "\n",
    "n_batch = create_batch_file(file_name='../sum_data/cnn.txt', batch_size=batch_size)\n",
    "print 'The number of batches: {0}'.format(n_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

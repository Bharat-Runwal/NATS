{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seqAttention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_emb_dim=100,\n",
    "        trg_emb_dim=100,\n",
    "        src_hidden_dim=25,\n",
    "        trg_hidden_dim=50,\n",
    "        src_vocab_size=999,\n",
    "        trg_vocab_size=999,\n",
    "        src_pad_token=0,\n",
    "        trg_pad_token=0,\n",
    "        src_nlayer=2,\n",
    "        trg_nlayer=1,\n",
    "        src_bidirect=True,\n",
    "        batch_size=128,\n",
    "        dropout=0.0\n",
    "    ):\n",
    "        super(seq2seqAttention, self).__init__()\n",
    "        # source embedding\n",
    "        self.src_embedding = torch.nn.Embedding(\n",
    "            src_vocab_size,\n",
    "            src_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.src_embedding.weight, \n",
    "            mean=0.0, \n",
    "            std=0.02\n",
    "        )\n",
    "        # targe embedding\n",
    "        self.trg_embedding = torch.nn.Embedding(\n",
    "            trg_vocab_size,\n",
    "            trg_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.normal(\n",
    "            self.trg_embedding.weight,\n",
    "            mean=0.0,\n",
    "            std=0.02\n",
    "        )\n",
    "        \n",
    "    def forward(self):\n",
    "        \n",
    "        return\n",
    "    \n",
    "model = seq2seqAttention(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=25,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=len(vocab2id),\n",
    "    trg_vocab_size=len(vocab2id),\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    src_bidirect=True,\n",
    "    batch_size=batch_size,\n",
    "    dropout=0.0\n",
    ").cuda()\n",
    "\n",
    "print model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size: 80475\n",
      "The number of batches: 1444\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../sum_data/'\n",
    "file_vocab = 'cnn_vocab.txt'\n",
    "file_corpus = 'cnn.txt'\n",
    "n_epoch = 20\n",
    "batch_size = 64\n",
    "\n",
    "vocab2id, id2vocab = construct_vocab(data_dir+'/'+file_vocab)\n",
    "print 'The vocabulary size: {0}'.format(len(vocab2id))\n",
    "\n",
    "n_batch = create_batch_file(file_name='../sum_data/cnn.txt', batch_size=batch_size)\n",
    "print 'The number of batches: {0}'.format(n_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size: 80475\n",
      "The number of batches: 1444\n",
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63\n",
      "--------------------------------------------------\n",
      "<unk> friend watch page life arabia the says <unk> <unk> is <unk> first the the world the the the the but world the the world the the world first who the he the the the world the the world first who the world first be he the the the world the the world the .\n",
      "--------------------------------------------------\n",
      "son of ted hughes and sylvia plath commits suicide , his sister says . nicholas hughes , whose mother gassed herself , hanged himself in alaska . hughes , 47 , was unmarried with no children of his own and was marine <unk> . </s>\n",
      "--------------------------------------------------\n",
      "<s> the family history of poets ted hughes and sylvia plath took another tragic turn monday when it was revealed that their son had committed suicide after battling depression . poets ted hughes and sylvia plath had separated before their son 's first birthday . nicholas hughes , whose mother asphyxiated herself in 1963 by putting her head in a gas oven at her london home while her two children slept in the next room , hanged himself at his home in alaska , his sister frieda told the times newspaper . hughes , 47 , was unmarried with no children of his own and had until recently been a marine biologist at the university of alaska fairbanks . the times said that shortly before his death he had left his academic job to set up a pottery workshop at home . frieda hughes , a poet , author and artist , said in a statement to the newspaper : `` it is with profound sorrow that i must announce the death of my brother , nicholas hughes , who died by his own hand on monday march 16 , 2009 at his home in alaska . `` he had been battling depression for some time . '' she added : `` his lifelong fascination with fish and fishing was a strong and shared bond with our father ( many of whose poems were about the natural world ) . `` he was a loving brother , a loyal friend to those who knew him and , despite the vagaries that life threw at him , he maintained an almost childlike innocence and enthusiasm for the next project or plan . '' hughes was only a baby when his mother killed herself , and his father tried to shield his children from the intense public interest in the family . some feminist groups blamed the death on ted hughes , who had left plath for <unk> <unk> , the wife of another poet . six years later , <unk> gassed herself and their daughter shura in an apparent copycat suicide . ted hughes died in 1998 , the year he published birthday letters , a series of poems about his life with plath and her death . hughes appears in both of his parents ' poetry . in `` nick and the candlestick , '' published in plath 's posthumous collection `` ariel , '' she wrote : `` you are the one . solid the spaces lean on , envious . you are the baby in the barn . '' later his father wrote of how , after plath 's death , their son 's eyes `` became wet jewels , the hardest substance of the purest pain . as i fed him in his high white chair . '' frieda hughes has written about her parents and her own battles with depression but a family friend dismissed the idea that nicolas 's death fitted into a family trend . `` nick was n't just the baby son of plath\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import shutil\n",
    "import re\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from lstm2lstm import *\n",
    "from data_utils import *\n",
    "\n",
    "data_dir = '../sum_data/'\n",
    "file_vocab = 'cnn_vocab.txt'\n",
    "file_corpus = 'cnn.txt'\n",
    "n_epoch = 20\n",
    "batch_size = 64\n",
    "\n",
    "vocab2id, id2vocab = construct_vocab(data_dir+'/'+file_vocab)\n",
    "print 'The vocabulary size: {0}'.format(len(vocab2id))\n",
    "\n",
    "n_batch = create_batch_file(file_name='../sum_data/cnn.txt', batch_size=batch_size)\n",
    "print 'The number of batches: {0}'.format(n_batch)\n",
    "\n",
    "model = seq2seq(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=25,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=len(vocab2id),\n",
    "    trg_vocab_size=len(vocab2id),\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    src_bidirect=True,\n",
    "    batch_size=batch_size,\n",
    "    dropout=0.0\n",
    ").cuda()\n",
    "\n",
    "model.load_state_dict(torch.load('../sum_data/lstm2lstm_results/lstm2lstm_19_1400.model'))\n",
    "\n",
    "src_var, trg_input_var, trg_output_var = process_minibatch(\n",
    "    100, vocab2id, max_lens=[512, 64]\n",
    ")\n",
    "\n",
    "trg_input_arr = [[vocab2id['<s>'] for k in range(64)] for j in range(batch_size)]\n",
    "trg_input_var = Variable(torch.LongTensor(trg_input_arr))\n",
    "for k in range(64):\n",
    "    print k,\n",
    "    logits = model(src_var.cuda(), trg_input_var.cuda())\n",
    "    word_prob = model.decode(logits).data.cpu().numpy().argmax(axis=-1)\n",
    "    for j in range(64):\n",
    "        trg_input_arr[j][k] = word_prob[j][k]\n",
    "        trg_input_var = Variable(torch.LongTensor(trg_input_arr))\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

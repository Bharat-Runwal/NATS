{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionBahdanau (\n",
      "  (softmax_): Softmax ()\n",
      "  (tanh_): Tanh ()\n",
      ")\n",
      "torch.Size([128, 100])\n",
      "torch.Size([128, 16, 100])\n",
      "Variable containing:\n",
      " 0.3733  0.3695  0.4731  ...   0.4381  0.4635  0.4432\n",
      " 0.5780  0.4864  0.4882  ...   0.5374  0.3941  0.4720\n",
      " 0.3845  0.5741  0.3348  ...   0.5652  0.4516  0.6332\n",
      "          ...             ⋱             ...          \n",
      " 0.6412  0.3669  0.5900  ...   0.5816  0.6887  0.4371\n",
      " 0.3454  0.5851  0.5945  ...   0.5130  0.5433  0.5959\n",
      " 0.4553  0.5663  0.6107  ...   0.6211  0.5329  0.5089\n",
      "[torch.cuda.FloatTensor of size 128x100 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.0722  0.0440  0.0019  ...   0.0197  0.1515  0.0038\n",
      " 0.1378  0.0390  0.1063  ...   0.2416  0.0713  0.0255\n",
      " 0.0283  0.0121  0.0463  ...   0.0022  0.0164  0.0019\n",
      "          ...             ⋱             ...          \n",
      " 0.1872  0.0138  0.0084  ...   0.0043  0.0227  0.0002\n",
      " 0.0409  0.0240  0.0231  ...   0.5090  0.0159  0.0031\n",
      " 0.2112  0.0165  0.1647  ...   0.0032  0.2381  0.0456\n",
      "[torch.cuda.FloatTensor of size 128x16 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AttentionBahdanau(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttentionBahdanau, self).__init__()   \n",
    "        \n",
    "        self.softmax_ = torch.nn.Softmax().cuda()\n",
    "        self.tanh_ = torch.nn.Tanh().cuda()\n",
    "        \n",
    "    def forward(self, last_dehy, enhy):\n",
    "        dehy_new = last_dehy.unsqueeze(2)\n",
    "\n",
    "        attn = torch.bmm(enhy, dehy_new).squeeze(2)\n",
    "        attn = self.softmax_(attn)\n",
    "        attn2 = attn.view(attn.size(0), 1, attn.size(1))\n",
    "        h_attn = torch.bmm(attn2, enhy).squeeze(1)\n",
    "        h_attn = self.tanh_(h_attn)\n",
    "\n",
    "        return h_attn, attn\n",
    "\n",
    "rd1 = Variable(torch.FloatTensor(torch.rand([128, 100]))).cuda()\n",
    "rd2 = Variable(torch.FloatTensor(torch.rand([128, 16, 100]))).cuda()\n",
    "model = AttentionBahdanau().cuda()\n",
    "print model\n",
    "print rd1.size()\n",
    "print rd2.size()\n",
    "hh, cc = model(rd1, rd2)\n",
    "print hh\n",
    "print cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionLuong (\n",
      "  (softmax_): Softmax ()\n",
      "  (tanh_): Tanh ()\n",
      "  (attn_out): Linear (200 -> 100)\n",
      ")\n",
      "torch.Size([128, 100])\n",
      "torch.Size([128, 16, 100])\n",
      "Variable containing:\n",
      "-2.9063e-01 -3.6004e-01 -4.0996e-02  ...  -7.4396e-02 -1.0122e-02 -3.3847e-01\n",
      " 2.1977e-02 -9.3524e-02 -2.6729e-02  ...   9.6436e-02  9.9872e-02 -8.2764e-02\n",
      "-1.4010e-01 -3.0645e-01 -8.5560e-02  ...  -1.3069e-01  6.2832e-02 -1.6895e-01\n",
      "                ...                   ⋱                   ...                \n",
      "-2.4782e-01 -1.3179e-01  1.6869e-01  ...  -9.0429e-02 -2.9188e-01 -4.1394e-01\n",
      "-1.2256e-01 -2.4674e-02  5.0530e-02  ...   1.6062e-01 -2.1412e-01 -2.5449e-01\n",
      "-1.2608e-03 -1.1116e-01 -1.6010e-01  ...  -1.1290e-02 -2.4701e-02 -2.1226e-01\n",
      "[torch.cuda.FloatTensor of size 128x100 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.0077  0.0139  0.0888  ...   0.0784  0.0123  0.0188\n",
      " 0.0308  0.0108  0.1964  ...   0.0202  0.0120  0.0167\n",
      " 0.0686  0.0098  0.1259  ...   0.0134  0.0525  0.0387\n",
      "          ...             ⋱             ...          \n",
      " 0.0022  0.0010  0.1586  ...   0.0993  0.0440  0.0178\n",
      " 0.0049  0.0063  0.0087  ...   0.0078  0.0210  0.0014\n",
      " 0.0605  0.0164  0.5756  ...   0.1110  0.0463  0.0082\n",
      "[torch.cuda.FloatTensor of size 128x16 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AttentionLuong(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        attn_method='luong_dot',\n",
    "        hidden_size=100,\n",
    "        bias=False\n",
    "    ):\n",
    "        super(AttentionLuong, self).__init__()\n",
    "        self.method = attn_method.lower()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.softmax_ = torch.nn.Softmax().cuda()\n",
    "        self.tanh_ = torch.nn.Tanh().cuda()\n",
    "        \n",
    "        if self.method == 'luong_concat':\n",
    "            self.attn_in = torch.nn.Sequential(\n",
    "                torch.nn.Linear(\n",
    "                    self.hidden_size*2,\n",
    "                    self.hidden_size,\n",
    "                    bias=self.bias\n",
    "                ),\n",
    "                torch.nn.Linear(self.hidden_size, 1, bias=self.bias)\n",
    "            ).cuda()\n",
    "        else:\n",
    "            if self.method == 'luong_general':\n",
    "                self.attn_in = torch.nn.Linear(\n",
    "                    self.hidden_size, \n",
    "                    self.hidden_size,\n",
    "                    bias=self.bias\n",
    "                ).cuda()\n",
    "                \n",
    "        self.attn_out = torch.nn.Linear(\n",
    "            self.hidden_size*2,\n",
    "            self.hidden_size,\n",
    "            bias=self.bias\n",
    "        ).cuda()\n",
    "        \n",
    "    def forward(self, dehy, enhy):\n",
    "        dehy_new = dehy.unsqueeze(2)\n",
    "        enhy_new = enhy\n",
    "        \n",
    "        if self.method == 'luong_concat':\n",
    "            dehy_rep = dehy.unsqueeze(1)\n",
    "            dehy_rep = dehy_rep.repeat(1, enhy.size(1), 1)\n",
    "            cat_hy = torch.cat((enhy, dehy_rep), 2)\n",
    "            attn = self.attn_in(cat_hy).squeeze(2)\n",
    "        else:\n",
    "            if self.method == 'luong_general':\n",
    "                enhy_new = self.attn_in(enhy)\n",
    "        \n",
    "            attn = torch.bmm(enhy_new, dehy_new).squeeze(2)\n",
    "        \n",
    "        attn = self.softmax_(attn)\n",
    "        attn2 = attn.view(attn.size(0), 1, attn.size(1))\n",
    "\n",
    "        attn_enhy = torch.bmm(attn2, enhy_new).squeeze(1)\n",
    "        \n",
    "        h_attn = self.attn_out(torch.cat((attn_enhy, dehy), 1))\n",
    "        h_attn = self.tanh_(h_attn)\n",
    "\n",
    "        return h_attn, attn\n",
    "\n",
    "rd1 = Variable(torch.FloatTensor(torch.rand([128, 100]))).cuda()\n",
    "rd2 = Variable(torch.FloatTensor(torch.rand([128, 16, 100]))).cuda()\n",
    "model = AttentionLuong(hidden_size=100).cuda()\n",
    "print model\n",
    "print rd1.size()\n",
    "print rd2.size()\n",
    "hh, cc = model(rd1, rd2)\n",
    "print hh\n",
    "print cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMDecoder (\n",
      "  (softmax_): Softmax ()\n",
      "  (tanh_): Tanh ()\n",
      "  (sigmoid_): Sigmoid ()\n",
      "  (lstm_): LSTMCell(150, 50)\n",
      "  (attn_layer): AttentionLuong (\n",
      "    (softmax_): Softmax ()\n",
      "    (tanh_): Tanh ()\n",
      "    (attn_in): Sequential (\n",
      "      (0): Linear (100 -> 50)\n",
      "      (1): Linear (50 -> 1)\n",
      "    )\n",
      "    (attn_out): Linear (100 -> 50)\n",
      "  )\n",
      ")\n",
      "torch.Size([128, 16, 50])\n"
     ]
    }
   ],
   "source": [
    "class LSTMDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers=1,\n",
    "        attn_method='bahdanau',\n",
    "        batch_first=True\n",
    "    ):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.attn_method = attn_method.lower()\n",
    "        \n",
    "        self.softmax_ = torch.nn.Softmax().cuda()\n",
    "        self.tanh_ = torch.nn.Tanh().cuda()\n",
    "        self.sigmoid_ = torch.nn.Sigmoid().cuda()\n",
    "        \n",
    "        if self.attn_method == 'vanilla':\n",
    "            self.lstm_ = torch.nn.LSTMCell(\n",
    "                self.input_size, \n",
    "                self.hidden_size\n",
    "            )\n",
    "        elif self.attn_method == 'bahdanau':\n",
    "            self.lstm_ = torch.nn.LSTMCell(\n",
    "                self.input_size+self.hidden_size, \n",
    "                self.hidden_size\n",
    "            )\n",
    "            self.attn_layer = AttentionBahdanau().cuda()\n",
    "            \n",
    "        else:\n",
    "            self.lstm_ = torch.nn.LSTMCell(\n",
    "                self.input_size+self.hidden_size, \n",
    "                self.hidden_size\n",
    "            )\n",
    "            self.attn_layer = AttentionLuong(\n",
    "                attn_method=self.attn_method, \n",
    "                hidden_size=self.hidden_size\n",
    "            ).cuda()\n",
    "        \n",
    "    def forward(self, input_, hidden_, encoder_hy):\n",
    "            \n",
    "        if self.batch_first:\n",
    "            input_ = input_.transpose(0,1)\n",
    "            \n",
    "        output_ = []\n",
    "        if self.attn_method == 'vanilla':\n",
    "            for k in range(input_.size(0)):\n",
    "                hidden_ = self.lstm_(input_[k], hidden_)\n",
    "                output_.append(hidden_[0])\n",
    "                \n",
    "        elif self.attn_method == 'bahdanau':\n",
    "            for k in range(input_.size(0)):\n",
    "                h_attn, attn = self.attn_layer(hidden_[0], encoder_hy.transpose(0,1))\n",
    "                x_input = torch.cat((input_[k], h_attn), 1)\n",
    "                hidden_ = self.lstm_(x_input, hidden_)\n",
    "                output_.append(hidden_[0])\n",
    "        else:\n",
    "            batch_size = input_.size(1)\n",
    "            h_attn = Variable(\n",
    "                torch.FloatTensor(torch.zeros(batch_size, self.hidden_size))\n",
    "            ).cuda()\n",
    "            for k in range(input_.size(0)):\n",
    "                x_input = torch.cat((input_[k], h_attn), 1)\n",
    "                hidden_ = self.lstm_(x_input, hidden_)\n",
    "                h_attn, attn = self.attn_layer(hidden_[0], encoder_hy.transpose(0,1))\n",
    "                output_.append(hidden_[0])\n",
    "            \n",
    "        len_seq = input_.size(0)\n",
    "        batch_size, hidden_size = output_[0].size()\n",
    "        output_ = torch.cat(output_, 0).view(\n",
    "            len_seq, \n",
    "            batch_size, \n",
    "            hidden_size\n",
    "        )\n",
    "        \n",
    "        if self.batch_first:\n",
    "            output_ = output_.transpose(0,1)\n",
    "            \n",
    "        return output_, hidden_\n",
    "    \n",
    "rd1 = Variable(torch.FloatTensor(torch.rand([128, 16, 100]))).cuda()\n",
    "hd2 = Variable(torch.FloatTensor(torch.rand([128, 50]))).cuda()\n",
    "cd2 = Variable(torch.FloatTensor(torch.rand([128, 50]))).cuda()\n",
    "hid2 = (hd2, cd2)\n",
    "ctx = Variable(torch.rand([16, 128, 50])).cuda()\n",
    "model = LSTMDecoder(\n",
    "    input_size=100, \n",
    "    hidden_size=50,\n",
    "    num_layers=1,\n",
    "    batch_first=True,\n",
    "    attn_method='Luong_concat'\n",
    ").cuda()\n",
    "print model\n",
    "aa, bb = model(rd1, hid2, ctx)\n",
    "print aa.size()\n",
    "#print bb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUDecoder (\n",
      "  (softmax_): Softmax ()\n",
      "  (tanh_): Tanh ()\n",
      "  (sigmoid_): Sigmoid ()\n",
      "  (gru_): GRUCell(150, 50)\n",
      "  (attn_layer): AttentionLuong (\n",
      "    (softmax_): Softmax ()\n",
      "    (tanh_): Tanh ()\n",
      "    (attn_out): Linear (100 -> 50)\n",
      "  )\n",
      ")\n",
      "torch.Size([128, 16, 50])\n"
     ]
    }
   ],
   "source": [
    "class GRUDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers=1,\n",
    "        attn_method='bahdanau',\n",
    "        batch_first=True\n",
    "    ):\n",
    "        super(GRUDecoder, self).__init__()\n",
    "        # parameters\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layer = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.attn_method = attn_method.lower()\n",
    "        \n",
    "        self.softmax_ = torch.nn.Softmax().cuda()\n",
    "        self.tanh_ = torch.nn.Tanh().cuda()\n",
    "        self.sigmoid_ = torch.nn.Sigmoid().cuda()\n",
    "        \n",
    "        if self.attn_method == 'vanilla':\n",
    "            self.gru_ = torch.nn.GRUCell(\n",
    "                self.input_size, \n",
    "                self.hidden_size\n",
    "            )\n",
    "        elif self.attn_method == 'bahdanau':\n",
    "            self.gru_ = torch.nn.GRUCell(\n",
    "                self.input_size+self.hidden_size, \n",
    "                self.hidden_size\n",
    "            )\n",
    "            self.attn_layer = AttentionBahdanau().cuda()\n",
    "            \n",
    "        else:\n",
    "            self.gru_ = torch.nn.GRUCell(\n",
    "                self.input_size+self.hidden_size, \n",
    "                self.hidden_size\n",
    "            )\n",
    "            self.attn_layer = AttentionLuong(\n",
    "                attn_method=self.attn_method, \n",
    "                hidden_size=self.hidden_size\n",
    "            ).cuda()\n",
    "        \n",
    "    def forward(self, input_, hidden_, encoder_hy):\n",
    "            \n",
    "        if self.batch_first:\n",
    "            input_ = input_.transpose(0,1)\n",
    "            \n",
    "        output_ = []\n",
    "        if self.attn_method == 'vanilla':\n",
    "            for k in range(input_.size(0)):\n",
    "                hidden_ = self.gru_(input_[k], hidden_)\n",
    "                output_.append(hidden_)\n",
    "                \n",
    "        elif self.attn_method == 'bahdanau':\n",
    "            for k in range(input_.size(0)):\n",
    "                h_attn, attn = self.attn_layer(hidden_, encoder_hy.transpose(0,1))\n",
    "                x_input = torch.cat((input_[k], h_attn), 1)\n",
    "                hidden_ = self.gru_(x_input, hidden_)\n",
    "                output_.append(hidden_)\n",
    "        else:\n",
    "            batch_size = input_.size(1)\n",
    "            h_attn = Variable(\n",
    "                torch.FloatTensor(torch.zeros(batch_size, self.hidden_size))\n",
    "            ).cuda()\n",
    "            for k in range(input_.size(0)):\n",
    "                x_input = torch.cat((input_[k], h_attn), 1)\n",
    "                hidden_ = self.gru_(x_input, hidden_)\n",
    "                h_attn, attn = self.attn_layer(hidden_, encoder_hy.transpose(0,1))\n",
    "                output_.append(hidden_)\n",
    "            \n",
    "        len_seq = input_.size(0)\n",
    "        batch_size, hidden_size = output_[0].size()\n",
    "        output_ = torch.cat(output_, 0).view(\n",
    "            len_seq, \n",
    "            batch_size, \n",
    "            hidden_size\n",
    "        )\n",
    "        \n",
    "        if self.batch_first:\n",
    "            output_ = output_.transpose(0,1)\n",
    "            \n",
    "        return output_, hidden_\n",
    "    \n",
    "rd1 = Variable(torch.FloatTensor(torch.rand([128, 16, 100]))).cuda()\n",
    "hd2 = Variable(torch.FloatTensor(torch.rand([128, 50]))).cuda()\n",
    "cd2 = Variable(torch.FloatTensor(torch.rand([128, 50]))).cuda()\n",
    "hid2 = (hd2, cd2)\n",
    "ctx = Variable(torch.rand([16, 128, 50])).cuda()\n",
    "model = GRUDecoder(\n",
    "    input_size=100, \n",
    "    hidden_size=50,\n",
    "    num_layers=1,\n",
    "    batch_first=True,\n",
    "    attn_method='Luong'\n",
    ").cuda()\n",
    "print model\n",
    "aa, bb = model(rd1, hd2, ctx)\n",
    "print aa.size()\n",
    "#print bb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq (\n",
      "  (softmax_): Softmax ()\n",
      "  (tanh_): Tanh ()\n",
      "  (sigmoid_): Sigmoid ()\n",
      "  (embedding): Embedding(999, 100, padding_idx=0)\n",
      "  (encoder): GRU(100, 25, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (decoder): GRUDecoder (\n",
      "    (softmax_): Softmax ()\n",
      "    (tanh_): Tanh ()\n",
      "    (sigmoid_): Sigmoid ()\n",
      "    (gru_): GRUCell(100, 50)\n",
      "  )\n",
      "  (encoder2decoder): Linear (50 -> 50)\n",
      "  (decoder2vocab): Linear (50 -> 999)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  1.1017  1.2107  1.0278  ...   0.9625  1.0906  1.0502\n",
       "  1.1098  1.2375  1.0092  ...   0.9213  1.1027  1.0541\n",
       "  1.0959  1.2471  0.9855  ...   0.9034  1.1090  1.0555\n",
       "           ...             ⋱             ...          \n",
       "  1.0524  1.2457  0.9323  ...   0.9058  1.1511  1.0484\n",
       "  1.0524  1.2457  0.9322  ...   0.9060  1.1514  1.0483\n",
       "  1.0524  1.2456  0.9321  ...   0.9062  1.1517  1.0483\n",
       "\n",
       "( 1 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  1.1017  1.2107  1.0278  ...   0.9625  1.0906  1.0502\n",
       "  1.1098  1.2375  1.0092  ...   0.9213  1.1027  1.0541\n",
       "  1.0959  1.2471  0.9855  ...   0.9034  1.1090  1.0555\n",
       "           ...             ⋱             ...          \n",
       "  1.0524  1.2457  0.9323  ...   0.9058  1.1511  1.0484\n",
       "  1.0524  1.2457  0.9322  ...   0.9060  1.1514  1.0483\n",
       "  1.0524  1.2456  0.9321  ...   0.9062  1.1517  1.0483\n",
       "\n",
       "( 2 ,.,.) = \n",
       "1.00000e-03 *\n",
       "  1.1017  1.2107  1.0278  ...   0.9625  1.0906  1.0502\n",
       "  1.1098  1.2375  1.0092  ...   0.9213  1.1027  1.0541\n",
       "  1.0959  1.2471  0.9855  ...   0.9034  1.1090  1.0555\n",
       "           ...             ⋱             ...          \n",
       "  1.0524  1.2457  0.9323  ...   0.9058  1.1511  1.0484\n",
       "  1.0524  1.2457  0.9322  ...   0.9060  1.1514  1.0483\n",
       "  1.0524  1.2456  0.9321  ...   0.9062  1.1517  1.0483\n",
       "... \n",
       "\n",
       "(125,.,.) = \n",
       "1.00000e-03 *\n",
       "  1.1017  1.2107  1.0278  ...   0.9625  1.0906  1.0502\n",
       "  1.1098  1.2375  1.0092  ...   0.9213  1.1027  1.0541\n",
       "  1.0959  1.2471  0.9855  ...   0.9034  1.1090  1.0555\n",
       "           ...             ⋱             ...          \n",
       "  1.0524  1.2457  0.9323  ...   0.9058  1.1511  1.0484\n",
       "  1.0524  1.2457  0.9322  ...   0.9060  1.1514  1.0483\n",
       "  1.0524  1.2456  0.9321  ...   0.9062  1.1517  1.0483\n",
       "\n",
       "(126,.,.) = \n",
       "1.00000e-03 *\n",
       "  1.1017  1.2107  1.0278  ...   0.9625  1.0906  1.0502\n",
       "  1.1098  1.2375  1.0092  ...   0.9213  1.1027  1.0541\n",
       "  1.0959  1.2471  0.9855  ...   0.9034  1.1090  1.0555\n",
       "           ...             ⋱             ...          \n",
       "  1.0524  1.2457  0.9323  ...   0.9058  1.1511  1.0484\n",
       "  1.0524  1.2457  0.9322  ...   0.9060  1.1514  1.0483\n",
       "  1.0524  1.2456  0.9321  ...   0.9062  1.1517  1.0483\n",
       "\n",
       "(127,.,.) = \n",
       "1.00000e-03 *\n",
       "  1.1017  1.2107  1.0278  ...   0.9625  1.0906  1.0502\n",
       "  1.1098  1.2375  1.0092  ...   0.9213  1.1027  1.0541\n",
       "  1.0959  1.2471  0.9855  ...   0.9034  1.1090  1.0555\n",
       "           ...             ⋱             ...          \n",
       "  1.0524  1.2457  0.9323  ...   0.9058  1.1511  1.0484\n",
       "  1.0524  1.2457  0.9322  ...   0.9060  1.1514  1.0483\n",
       "  1.0524  1.2456  0.9321  ...   0.9062  1.1517  1.0483\n",
       "[torch.cuda.FloatTensor of size 128x18x999 (GPU 0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        src_emb_dim=100,\n",
    "        trg_emb_dim=100,\n",
    "        src_hidden_dim=50,\n",
    "        trg_hidden_dim=50,\n",
    "        src_vocab_size=999,\n",
    "        trg_vocab_size=999,\n",
    "        src_pad_token=0,\n",
    "        trg_pad_token=0,\n",
    "        src_nlayer=1,\n",
    "        trg_nlayer=1,\n",
    "        batch_first=True,\n",
    "        src_bidirect=True,\n",
    "        batch_size=128,\n",
    "        dropout=0.0,\n",
    "        attn_method='vanilla',\n",
    "        network_='gru'\n",
    "    ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        # parameters\n",
    "        self.src_emb_dim = src_emb_dim\n",
    "        self.trg_emb_dim = trg_emb_dim\n",
    "        self.src_hidden_dim = src_hidden_dim\n",
    "        self.trg_hidden_dim = trg_hidden_dim\n",
    "        self.src_vocab_size = src_vocab_size\n",
    "        self.trg_vocab_size = trg_vocab_size\n",
    "        self.src_nlayer = src_nlayer\n",
    "        self.trg_nlayer = trg_nlayer\n",
    "        self.batch_first = batch_first\n",
    "        self.src_bidirect = src_bidirect\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.attn_method = attn_method\n",
    "        self.network_ = network_.lower()\n",
    "        \n",
    "        self.softmax_ = torch.nn.Softmax().cuda()\n",
    "        self.tanh_ = torch.nn.Tanh().cuda()\n",
    "        self.sigmoid_ = torch.nn.Sigmoid().cuda()\n",
    "        \n",
    "        self.src_num_directions = 1\n",
    "        if self.src_bidirect:\n",
    "            self.src_hidden_dim = src_hidden_dim // 2\n",
    "            self.src_num_directions = 2\n",
    "        \n",
    "        # source embedding and target embedding\n",
    "        # the same for summarization.\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            self.src_vocab_size,\n",
    "            self.src_emb_dim,\n",
    "            padding_idx=0\n",
    "        ).cuda()\n",
    "        torch.nn.init.uniform(self.embedding.weight, -1.0, 1.0)\n",
    "        # choose network\n",
    "        if self.network_ == 'lstm':\n",
    "            # encoder\n",
    "            self.encoder = torch.nn.LSTM(\n",
    "                input_size=self.src_emb_dim,\n",
    "                hidden_size=self.src_hidden_dim,\n",
    "                num_layers=self.src_nlayer,\n",
    "                batch_first=self.batch_first,\n",
    "                dropout=self.dropout,\n",
    "                bidirectional=self.src_bidirect\n",
    "            ).cuda()\n",
    "            # decoder\n",
    "            self.decoder = LSTMDecoder(\n",
    "                input_size=self.trg_emb_dim,\n",
    "                hidden_size=self.trg_hidden_dim,\n",
    "                batch_first=self.batch_first,\n",
    "                attn_method=self.attn_method\n",
    "            ).cuda()\n",
    "        else:\n",
    "            # encoder\n",
    "            self.encoder = torch.nn.GRU(\n",
    "                input_size=self.src_emb_dim,\n",
    "                hidden_size=self.src_hidden_dim,\n",
    "                num_layers=self.src_nlayer,\n",
    "                batch_first=self.batch_first,\n",
    "                dropout=self.dropout,\n",
    "                bidirectional=self.src_bidirect\n",
    "            ).cuda()\n",
    "            # decoder\n",
    "            self.decoder = GRUDecoder(\n",
    "                input_size=self.trg_emb_dim,\n",
    "                hidden_size=self.trg_hidden_dim,\n",
    "                batch_first=self.batch_first,\n",
    "                attn_method=self.attn_method\n",
    "            ).cuda()\n",
    "            \n",
    "        # encoder to decoder\n",
    "        self.encoder2decoder = torch.nn.Linear(\n",
    "            self.src_hidden_dim*self.src_num_directions,\n",
    "            self.trg_hidden_dim\n",
    "        ).cuda()\n",
    "        torch.nn.init.constant(self.encoder2decoder.bias, 0.0)\n",
    "        # decoder to vocab\n",
    "        self.decoder2vocab = torch.nn.Linear(\n",
    "            self.trg_hidden_dim,\n",
    "            self.trg_vocab_size\n",
    "        ).cuda()\n",
    "        torch.nn.init.constant(self.decoder2vocab.bias, 0.0)\n",
    "        \n",
    "    def forward(self, input_src, input_trg):\n",
    "        src_emb = self.embedding(input_src)\n",
    "        trg_emb = self.embedding(input_trg)\n",
    "        \n",
    "        batch_size = input_src.size(1)\n",
    "        if self.batch_first:\n",
    "            batch_size = input_src.size(0)\n",
    "\n",
    "        h0_encoder = Variable(torch.zeros(\n",
    "            self.encoder.num_layers*self.src_num_directions,\n",
    "            self.batch_size,\n",
    "            self.src_hidden_dim\n",
    "        ), requires_grad=False).cuda()\n",
    "        \n",
    "        if self.network_ == 'lstm':\n",
    "            c0_encoder = Variable(torch.zeros(\n",
    "                self.encoder.num_layers*self.src_num_directions,\n",
    "                self.batch_size,\n",
    "                self.src_hidden_dim\n",
    "            ), requires_grad=False).cuda()\n",
    "\n",
    "            src_h, (src_h_t, src_c_t) = self.encoder(\n",
    "                src_emb, \n",
    "                (h0_encoder, c0_encoder)\n",
    "            )\n",
    "\n",
    "            if self.src_bidirect:\n",
    "                h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1)\n",
    "                c_t = torch.cat((src_c_t[-1], src_c_t[-2]), 1)\n",
    "            else:\n",
    "                h_t = src_h_t[-1]\n",
    "                c_t = src_c_t[-1]\n",
    "                        \n",
    "            decoder_h0 = self.encoder2decoder(h_t)\n",
    "            decoder_h0 = self.tanh_(decoder_h0)\n",
    "            decoder_c0 = c_t\n",
    "        \n",
    "            encoder_hy = src_h.transpose(0,1)\n",
    "        \n",
    "            trg_h, (_, _) = self.decoder(\n",
    "                trg_emb,\n",
    "                (decoder_h0, decoder_c0),\n",
    "                encoder_hy\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            src_h, src_h_t = self.encoder(\n",
    "                src_emb, \n",
    "                h0_encoder\n",
    "            )\n",
    "\n",
    "            if self.src_bidirect:\n",
    "                h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1)\n",
    "            else:\n",
    "                h_t = src_h_t[-1]\n",
    "                        \n",
    "            decoder_h0 = self.encoder2decoder(h_t)\n",
    "            decoder_h0 = self.tanh_(decoder_h0)\n",
    "        \n",
    "            encoder_hy = src_h.transpose(0,1)\n",
    "        \n",
    "            trg_h, _ = self.decoder(\n",
    "                trg_emb,\n",
    "                decoder_h0,\n",
    "                encoder_hy\n",
    "            )\n",
    "        \n",
    "        trg_h_reshape = trg_h.contiguous().view(\n",
    "            trg_h.size(0) * trg_h.size(1),\n",
    "            trg_h.size(2)\n",
    "        )\n",
    "                \n",
    "        decoder_output = self.decoder2vocab(trg_h_reshape)\n",
    "        decoder_output = decoder_output.view(\n",
    "            trg_h.size(0),\n",
    "            trg_h.size(1),\n",
    "            decoder_output.size(1)\n",
    "        )\n",
    "        \n",
    "        return decoder_output\n",
    "    \n",
    "    def decode(self, logits):\n",
    "        logits_reshape = logits.view(-1, self.trg_vocab_size)\n",
    "        word_probs = F.softmax(logits_reshape)\n",
    "        word_probs = word_probs.view(\n",
    "            logits.size(0), logits.size(1), logits.size(2)\n",
    "        )\n",
    "\n",
    "        return word_probs\n",
    "\n",
    "model = Seq2Seq(\n",
    "    src_emb_dim=100,\n",
    "    trg_emb_dim=100,\n",
    "    src_hidden_dim=50,\n",
    "    trg_hidden_dim=50,\n",
    "    src_vocab_size=999,\n",
    "    trg_vocab_size=999,\n",
    "    src_pad_token=0,\n",
    "    trg_pad_token=0,\n",
    "    src_nlayer=2,\n",
    "    trg_nlayer=1,\n",
    "    batch_first=True,\n",
    "    src_bidirect=True,\n",
    "    batch_size=128,\n",
    "    dropout=0.0,\n",
    "    attn_method='vanilla',\n",
    "    network_='gru'\n",
    ").cuda()\n",
    "\n",
    "print model\n",
    "\n",
    "sen_in = Variable(torch.LongTensor(128, 16).fill_(10))\n",
    "sen_out = Variable(torch.LongTensor(128, 18).fill_(9))\n",
    "out = model(sen_in.cuda(), sen_out.cuda())\n",
    "model.decode(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 5])\n",
      "torch.Size([3, 128, 5])\n",
      "torch.Size([128, 5])\n",
      "torch.Size([128, 5])\n",
      "torch.Size([128, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "rnn = torch.nn.LSTM(9, 5, num_layers=3, bidirectional=False, batch_first=True)\n",
    "\n",
    "input = Variable(torch.randn(128, 3, 9))\n",
    "h0 = Variable(torch.randn(3, 128, 5))\n",
    "c0 = Variable(torch.randn(3, 128, 5))\n",
    "output, hn = rnn(input, (h0, c0))\n",
    "hh, cc = hn\n",
    "print hh.size()\n",
    "print cc.size()\n",
    "print hh[-1].size()\n",
    "print cc[-1].size()\n",
    "print output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1  2\n",
      " 2  3\n",
      "[torch.LongTensor of size 2x2]\n",
      "\n",
      "\n",
      " 2  3\n",
      " 4  4\n",
      "[torch.LongTensor of size 2x2]\n",
      "\n",
      "\n",
      "  2   6\n",
      "  8  12\n",
      "[torch.LongTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "aa = torch.LongTensor([[1, 2], [2, 3]]) \n",
    "bb = torch.LongTensor([[2, 3], [4, 4]])\n",
    "print aa\n",
    "print bb\n",
    "print aa*bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
